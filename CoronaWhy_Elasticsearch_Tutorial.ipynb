{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CoronaWhy Elasticsearch Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonpolishko/A_colab_collection/blob/master/CoronaWhy_Elasticsearch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFhSuiiMWCJB",
        "colab_type": "text"
      },
      "source": [
        "# Example Elasticsearch Queries and Convenience Functions for http://search.coronawhy.org\n",
        "\n",
        "CoronaWhy has ingested the CORD-19 dataset into an Elasticsearch instance deployed on our server. You can use it to make queries, get relevant documents, do some data exploration, and check the quality of keywords (among other things). I can't provide a guide for everything possible with Elasticsearch, but in this notebook I'll try to cover the most common use cases so you can get started, without needing to learn an entirely new package and query language. \n",
        "\n",
        "## Insallation\n",
        "\n",
        "Make sure you're running python 3, and if you can, make sure in your virtualenv you've installed Elasticsearch with \n",
        "pip install elasticsearch==7.6.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAfykcG1WCJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "47d29717-f398-4773-a9fd-d0adf880ea8c"
      },
      "source": [
        "# Make sure you're running python3 for this, I haven't tested python2! \n",
        "!pip install elasticsearch==7.6.0\n",
        "\n",
        "from elasticsearch import helpers, Elasticsearch\n",
        "ESURL = \"http://elastic:changeme@search.coronawhy.org:80\"\n",
        "es = Elasticsearch(ESURL, Port=80) # Do not change this! "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elasticsearch==7.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/cf/7973ac58090b960857da04add0b345415bf1e1741beddf4cbe136b8ad174/elasticsearch-7.6.0-py2.py3-none-any.whl (88kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch==7.6.0) (1.24.3)\n",
            "Installing collected packages: elasticsearch\n",
            "Successfully installed elasticsearch-7.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h1nmpv1WCJH",
        "colab_type": "text"
      },
      "source": [
        "## What does the index look like? \n",
        "\n",
        "There are indexes at three levels: sentence, paragraph, and full article. They are:\n",
        "1. v9sentences (Sentence level annotations)\n",
        "2. v9sections (Paragraph level annotations)\n",
        "3. v9papers (Document level annotations)\n",
        "\n",
        "Which index you use will depend on your use case. If you want to find papers that are related to each other, use v9papers. If you want to find a paragraph of text that talks about PCR results or the effect of smoking on COVID-19 comorbidity, use v9sections. If you want only the number of patients involved in studies, use v9sentences and filter by keywords. \n",
        "\n",
        "As of writing this document, only v9sentences is available. We will make the other two available as soon as is possible. :) \n",
        "\n",
        "Please see the documentation here to learn about how the data is processed and which fields are available:\n",
        "\n",
        "https://drive.google.com/open?id=1FesFFx5LLrWCUTBLFBbTQKCwCWFGN8z7\n",
        "\n",
        "\n",
        "Finally, let's take a look at the fields available in our index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ0zEGMsWCJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# get mapping fields for a specific index:\n",
        "index = \"v9sentences\"\n",
        "#ESURL = \"http://elastic:changeme@search.coronawhy.org:80\"\n",
        "elastic_url = ESURL \n",
        "mapping_fields_request = \"_mapping/field/*?ignore_unavailable=false&allow_no_indices=false&include_defaults=true\"\n",
        "mapping_fields_url = \"/\".join([elastic_url, index, mapping_fields_request])\n",
        "print(mapping_fields_url)\n",
        "response = requests.get(mapping_fields_url)\n",
        "\n",
        "# parse the data:\n",
        "data = response.content.decode()\n",
        "parsed_data = json.loads(data)\n",
        "keys = sorted(parsed_data[index][\"mappings\"].keys())\n",
        "print(\"index= {} has a total of {} keys\".format(index, len(keys)))\n",
        "\n",
        "#keys of the fields:\n",
        "fields = [{i,key} for  i, key in enumerate(keys)]\n",
        "print([i for i in fields])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQI9cDdNWCJM",
        "colab_type": "text"
      },
      "source": [
        "# 87 keys!? Please read!\n",
        "\n",
        "Yes -- but most of these are duplicates. You need to undersand the difference between a text field and a keyword field. \n",
        "\n",
        "Text fields (\"language\", \"cord_uid\") are ingested into Elasticsearch and split up with spaces and analyzed. You can do basic search over these fields. \n",
        "\n",
        "Keyword fields (\"language.keyword\", \"cord_uid.keyword\") are fields that do not get analyzed. Lucky for you, we provide these fields already analyzed! We create lists of lemmas (basic word forms) for every sentence in the entire corpus, with common words, punctuation, and pure numbers are removed. That means instead of \n",
        "\n",
        "\"The 25 rocks are insanely big, and COVID-19 is bad\"... \n",
        "the lemma column has:\n",
        "\"rock, insane, big, covid-19, bad\"\n",
        "\n",
        "If you're looking for a specific number, use the non-keyword field. If you're looking for keywords, use the keyword field. It's also possible to use both! \n",
        "\n",
        "!!!!!PLEASE NOTE!!!!! UMLS, lemma, and all the other NER fields and fields that contain lists are ALREADY keyword fields. You cannot search them using sentences; they must be searched with lists. An example of this will be provided below.\n",
        "\n",
        "Every sentence has a unique identifier, sentence_id. For sections, that's section_id. For documents, that's cord_uid, which is the same cord_uid as in the dataset provided by AI2 in CORD-19. \n",
        "\n",
        "# The Search Methods\n",
        "\n",
        "There are several search methods I've written here to make your life more convenient when working with our ES instance. This is not exhaustive, but should get you started. For additional help, please use the Elasticsearch documentation to create queries: https://www.elastic.co/guide/en/elasticsearch/reference/current/full-text-queries.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqcQSvURWCJN",
        "colab_type": "text"
      },
      "source": [
        "## More LikeThis\n",
        "\n",
        "A More Like This (MLT) query will measure relevance according to the BM25 algorithm. \n",
        "\n",
        "Use this query if you're trying to answer the question: \"Are there other sentences, sections, or documents like this one?\"\n",
        "\n",
        "Keep in mind, if you're searching a keyword field, you should provide lemmatized text. We'll build this into our search engine later so you don't have to worry about it, so stay tuned for updates.\n",
        "\n",
        "Additional info here: https://qbox.io/blog/mlt-similar-documents-in-elasticsearch-more-like-this-query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdNCm_x8WCJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def more_like_this(query, match_phrase=\"\", index=\"v9sentences\", match_field=\"\", size=1000, fields=[\"sentence\"], min_term_freq=1, max_query_terms=12):\n",
        "    \n",
        "    # This function makes a query to ElasticSearch and returns the 1000 most\n",
        "    # similar documents based on: a query document, and a phrase that must\n",
        "    # occur in the article's main text. \n",
        "    #\n",
        "    # --VARIABLE DEFINITIONS--\n",
        "    # query: the main text you want to measure relevance against.\n",
        "    #        Can be a word, sentence, paragraph, or whole text.\n",
        "    #\n",
        "    # fields: Optional list of the fields you want to search in. Fulltext \n",
        "    #         searches only work with [\"sentence\"], while searching for lists\n",
        "    #         should work in most other fields (keyword fields)\n",
        "    #\n",
        "    # match_phrase: the phrase that must occur in the field's text\n",
        "    #\n",
        "    # match_field: The field that needs to match whatever your query is.\n",
        "    #\n",
        "    # match_phrase: Optional string. The search will ONLY return documents\n",
        "    #               where the whole phrase is matched.              \n",
        "    \n",
        "    if len(match_phrase):\n",
        "        search_body = {\n",
        "                    \"size\": size,\n",
        "                    \"query\": {\n",
        "                       \"bool\": {\n",
        "                          \"must\": [\n",
        "                            {\n",
        "                             \"more_like_this\": {\n",
        "                             \"fields\" : fields,\n",
        "                             \"like\" : query,\n",
        "                             \"min_term_freq\" : 1,\n",
        "                             \"max_query_terms\" : 12\n",
        "                         }\n",
        "                             },\n",
        "                             {\n",
        "                                \"match_phrase\": {\n",
        "                                   match_field: match_phrase\n",
        "                                }\n",
        "                             }\n",
        "                          ]\n",
        "                       }\n",
        "                    }\n",
        "                 }\n",
        "\n",
        "    else:\n",
        "        search_body = {\n",
        "              \"size\": size,\n",
        "               \"query\": {\n",
        "                        \"more_like_this\": {\n",
        "                        \"fields\" : fields,\n",
        "                        \"like\" : query,\n",
        "                        \"min_term_freq\" : 1,\n",
        "                        \"min_doc_freq\":1\n",
        "                        }    \n",
        "               }\n",
        "        }\n",
        "    \n",
        "    res = es.search(index=index, body=search_body)\n",
        "    return [hit[\"_source\"] for hit in res[\"hits\"][\"hits\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP9RCyYwWCJR",
        "colab_type": "text"
      },
      "source": [
        "### Test MLT\n",
        "\n",
        "Alright, let's say I want to find sentences related to comorbidity, and I only want 5 documents. Describe what you're looking for in a summarized way. Do not ask a question! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMDOBiVSWCJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = more_like_this(query=\"Comorbidity, death, coronavirus\", \n",
        "                         size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVOu9A_SWCJU",
        "colab_type": "text"
      },
      "source": [
        "How many hits did we get? We're expecting 5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX3Ns3LFWCJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz9fFMbRWCJY",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the top hit. But we only want to know what the sentence says, and the IDs for the UMLS entities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdwf_qWvWCJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(results[0][\"sentence\"])\n",
        "print(results[0][\"UMLS_IDS\"])\n",
        "print(results[0][\"lemma\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O1CSu48WCJc",
        "colab_type": "text"
      },
      "source": [
        "Excellent. Now let's get documents by matching a whole phrase instead of just single words. Now, we want to find \"survival without comorbidity\" in the sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFOrEp_vWCJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = more_like_this(query=\"Comorbidity, death, coronavirus\", \n",
        "                         match_field=\"sentence\",\n",
        "                         match_phrase=\"survival without comorbidity\",\n",
        "                         size=5)\n",
        "print(len(results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXU7u7vKWCJg",
        "colab_type": "text"
      },
      "source": [
        "Notice we wanted 5, but only got 4 results. That's because the phrase only occurs in 4 sentences within our corpus. Let's take a look at those sentences in order of MLT relevance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl8KABLfWCJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in results:\n",
        "    print(i[\"sentence\"] + \"\\n\" + i[\"sentence_id\"] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZjlPTS8WCJj",
        "colab_type": "text"
      },
      "source": [
        "Oho! Looks like we've found some duplicate sentences. Looks like some documents were indexed twice. Can we search keyword fields in the same way? Yep! (Ignore duplicates, we're doing our best!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBjK9cHGWCJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = more_like_this(fields=[\"UMLS\"],\n",
        "                         query=\"Comorbidity, death, coronavirus\",\n",
        "                         size=5)\n",
        "for i in results:\n",
        "    print(i[\"sentence\"] + \"\\n\" + i[\"sentence_id\"] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSk4_2jnWCJl",
        "colab_type": "text"
      },
      "source": [
        "## Using match_phrase to filter by cord_uid\n",
        "\n",
        "If we know the cord_uid of the paper we want to grab, and only want to search within that paper, we can provide the cord_uid as the match_phrase, and set match_field to \"cord_uid\" to achieve that functionality. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvLIIaGiWCJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = more_like_this(fields=[\"UMLS\"],\n",
        "                         query=\"Comorbidity, death, coronavirus\", \n",
        "                         match_field=\"cord_uid\",\n",
        "                         match_phrase=\"ow2xqhmp\",\n",
        "                         size=3)\n",
        "print(len(results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsBuauZeWCJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "71c697e6-77ca-4563-b896-329b8f85349a"
      },
      "source": [
        "for i in results:\n",
        "    print(i[\"sentence\"] + \"\\n\" + \n",
        "          \"Cord_uid:\" + f'\\x1b[31m{i[\"cord_uid\"]}\\x1b[0m' + \"\\n\" + \n",
        "          \"Sentence ID\" + i[\"cord_uid\"] + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ca7df04eac8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     print(i[\"sentence\"] + \"\\n\" + \n\u001b[1;32m      3\u001b[0m           \u001b[0;34m\"Cord_uid:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'\\x1b[31m{i[\"cord_uid\"]}\\x1b[0m'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \"Sentence ID\" + i[\"cord_uid\"] + \"\\n\")\n",
            "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6XFsvAzWCJr",
        "colab_type": "text"
      },
      "source": [
        "## Filter query\n",
        "\n",
        "If you know what you're looking for and you want to pull all documents that contain a specific word or term, use this type of search. We're not interested in relevance here, just a simple keyword filter! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cIFIPq8WCJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_filter(terms=\"covid-19\", field=\"UMLS\", size=10, match_all=False):\n",
        "    \n",
        "    # This method will search the data and return only data where the field\n",
        "    # contains the terms you're looking for. You can enter a string, or a \n",
        "    # list of strings, and the method will handle them accordingly. \n",
        "    \n",
        "    if isinstance(terms, str):\n",
        "        search_body= {\n",
        "            \"size\" : size,\n",
        "            \"query\": { \n",
        "                \"bool\": { \n",
        "                \"filter\": [ \n",
        "                    { \"term\":  { field: terms }},\n",
        "                    #{ \"range\": { \"publish_date\": { \"gte\": \"2015-01-01\" }}} # Get papers published after date\n",
        "                      ]\n",
        "                    }\n",
        "                  }\n",
        "                }\n",
        "    elif isinstance(terms, list):\n",
        "        if match_all==False:\n",
        "            search_body={\n",
        "                      \"query\": {\n",
        "                        \"bool\" : {\n",
        "                          \"must\" : {\n",
        "                              \"terms\" : {\n",
        "                                field : terms\n",
        "                              }\n",
        "                          }\n",
        "                        }\n",
        "                      }\n",
        "                    }\n",
        "        elif match_all==True:\n",
        "            search_body={\n",
        "                      \"query\": {\n",
        "                        \"bool\" : {\n",
        "                          \"must\" : {\n",
        "                              \"terms\" : {\n",
        "                                field : terms\n",
        "                              }\n",
        "                          },\n",
        "                            \"minimum_should_match\": len(terms)\n",
        "                        }\n",
        "                      }\n",
        "                    }\n",
        "    \n",
        "    else:\n",
        "        return(\"You need to provide a list or a string for the terms variable!\")\n",
        "\n",
        "    res = es.search(index=index, body=search_body)\n",
        "    \n",
        "    if len(res[\"hits\"][\"hits\"]) == 0:\n",
        "        print(\"No hits!\")\n",
        "    \n",
        "    return [hit[\"_source\"] for hit in res[\"hits\"][\"hits\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LFPcFBDWCJu",
        "colab_type": "text"
      },
      "source": [
        "### Cool (but problematic) fact about the UMLS column\n",
        "\n",
        "The UMLS column actually represents a normalized version of various concepts and named entities in the \"sentence\" column. That means if \"coronary\" or \"blood-pumping organ\" are present in the text, they'll get mapped to something like \"heart\" in UMLS! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrC00FsQWCJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = simple_filter(terms=[\"heart\", \"dead\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPRwiZEWCJw",
        "colab_type": "text"
      },
      "source": [
        "You can add the \"match all\" flag if you want to make sure ALL of your keywords are contained in the document. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCfFnbPRWCJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = simple_filter(field=\"lemma\", terms=[\"test\", \"heart\"], match_all=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6huMEgvWCJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_the_results(query, field=\"sentence\", index=\"v9sentences\"):\n",
        "    # Type a string query, get a list of all _id that match\n",
        "    # in the ES index\n",
        "    ESURL = \"http://elastic:changeme@search.coronawhy.org:80/\"\n",
        "    es = Elasticsearch(ESURL, Port=80, request_timeout=60) # Do not change this! \n",
        "    article_list = []\n",
        "    res = es.search(\n",
        "        index=index,\n",
        "        scroll='60s',\n",
        "        size=1000,\n",
        "        body={\n",
        "           \"query\": {\n",
        "                       \"match\": {\n",
        "                          field: query\n",
        "                       }\n",
        "                    }\n",
        "           }\n",
        "        )   \n",
        "\n",
        "    # Get the scroll ID\n",
        "    sid = res['_scroll_id']\n",
        "    scroll_size = len(res['hits']['hits'])\n",
        "    article_list.extend([hit['_source'] for hit in res['hits']['hits']])\n",
        "    \n",
        "    while scroll_size > 0:\n",
        "        res = es.scroll(scroll_id=sid, scroll='2m')\n",
        "\n",
        "        # Update the scroll ID\n",
        "        sid = res['_scroll_id']\n",
        "\n",
        "        # Get the number of results that returned in the last scroll\n",
        "        scroll_size = len(res['hits']['hits'])\n",
        "        article_list.extend([hit['_source'] for hit in res['hits']['hits']])\n",
        "    return article_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MumDWjuMWCJ0",
        "colab_type": "text"
      },
      "source": [
        "# Converting to a Pandas dataframe\n",
        "\n",
        "As I'm sure some of you will want to work with Pandas, I've written the code so it's as easy as 1, 2, 3! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fhz1ow7WCJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWEy3odzWCJ2",
        "colab_type": "text"
      },
      "source": [
        "Since you probably don't care about the vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPBTqrpsWCJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if 'w2vVector' in results_df:\n",
        "  results_df.drop(columns=[\"w2vVector\"], inplace=True)\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUseWSPWCJ5",
        "colab_type": "text"
      },
      "source": [
        "Have fun and good luck! "
      ]
    }
  ]
}